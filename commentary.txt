The Many-Analysts Religion Project (MARP) illustrates how researcher degrees of freedom cause one research question to be analyzed in many different ways. Each submission showcases a different perspective on “best practices”. My submission illustrates two practices I consider important: Firstly, how the Workflow for Open Reproducible Code in Science can be used to create a fully reproducible paper and an unambiguous preregistration (WORCS; Van Lissa et al., 2020). Secondly, how rigorous exploration can complement confirmatory (hypothesis-testing) research, even in a preregistered study. 

WORCS is a conceptual workflow based on three principles: 1) writing papers as dynamic documents that combine prose and analysis code and can be reproduced with a single click, 2) using version control to track every change to the project since its inception, and 3) managing dependencies, which means documenting all software required to reproduce the project. These principles are automatically implemented by an Rstudio project template in the worcs R-package. The WORCS-project for my MARP analyses is available at https://github.com/cjvanlissa/manyanalysts_religion. Following conventions, a preregistration form was submitted to OSF.io. Additionally, the state of the project at time of preregistration was tagged (like a time capsule). This so-called “Preregistration As Code” is arguably more comprehensive and unambiguous than a written preregistration form (Peikert, Van Lissa, & Brandmaier, 2021). It contains the exact analysis code that I intended to run, complete with a simulated dataset that allowed me to verify that the code worked as expected. No plan is perfect of course, so after receiving the real data, some adjustments were necessary. Reviewers and readers can see exactly what changes were made by comparing the finished project to the preregistered project (using Git diff; https://github.com/cjvanlissa/manyanalysts_religion/compare/1f2a6bd..e38391d). A changelog indicates why these changes were considered necessary. The finished project is reproducible: Third-party auditors can download the entire repository, and follow the reproduction procedure described here (https://cjvanlissa.github.io/worcs/articles/reproduce.html). WORCS provides solutions for specific challenges to reproducibility; for example, if original data cannot be shared, WORCS generates a synthetic dataset that allows auditors to at least verify the correctness of analysis code. WORCS is particularly suited for preregistered studies because it allows researchers to preregister a reproducible draft manuscript with functioning code for planned analyses, evaluated on a simulated dataset. Two key advantages over preregistration forms are that 1) it obviates the need to write two distinct documents in separate formats (preregistration versus manuscript); the preregistered manuscript can simply be updated once real data is collected or accessed, and 2) computer code is much less ambiguous than verbal descriptions of planned analyses, thus reducing researcher degrees of freedom.

The second practice I want to address is exploratory research. The open science revolution has prioritized confirmatory research; for instance, by advocating preregistration and replication. Nevertheless, exploratory research is an integral part of the “empirical cycle”: Data-driven insights inspire new hypotheses and help amend existing theory. Of course, it is crucial that confirmation and exploration are clearly demarcated; my preregistration explicates my intention to supplement planned hypothesis tests with exploratory research. Not all exploration is rigorous, however. Many researchers in the social sciences are trained in regression-based methods, and use this method for exploratory research by trying different combinations of predictor- and outcome variables until an interesting “significant effect” shows up. This is a labor-intensive and ill-suited method for exploration, for many reasons, including that p-values are not valid for model selection. Instead, why not make use of machine learning methods, which are specifically designed for exploration. Machine learning methods can be understood as techniques that identify patterns in data, while incorporating checks and balances to curtail false-positive findings and maximize generalizability to new data (Hastie, Tibshirani, & Friedman, 2009). These methods automate exploration and ensure robust results. Three results are most relevant for exploration: First, an estimate of the model’s predictive performance in new data. This establishes an upper bound for how well all of the variables included in the study are able to explain the outcome. If the variance explained by the theoretical model is close to that explained by a machine learning model, then the theoretical model might be quite good. If the discrepancy is large, the theoretical model can be improved. Finally, if this upper bound is low, that might give cause to rethink the study design. The second result is the rank-ordered variable importance of each predictor, which refers to that predictor’s relative contribution to the accuracy of the model’s predictions. Highly ranked variables make major contributions to a model’s predictive accuracy, and are thus important to consider when planning future studies or amending theory. The third result consists of the marginal associations of each predictor (or combination of predictors) with the outcome. These marginal associations can reveal non-linearity and even putative interactions.

My MARP submission used a specific machine learning algorithm called random forests. Results (https://github.com/cjvanlissa/manyanalysts_religion/blob/master/manuscript.md) indicated that, firstly, the predictive explained variance was R^2_{oob} = .28. This is relatively low; thus we must consider that some important predictors of wellbeing are omitted from the data, or that the wellbeing scale has high irreducible error. The latter seems unlikely due to the high reliability (α = .91).  Secondly, When examining variable importance (Figure 1), religiosity and perceived cultural norms do not emerge as the most important predictors of wellbeing. Instead, socio-economic status appears to be by far the most important predictor of wellbeing, followed by between-country differences. SES should thus be considered as a relevant covariant, or featured in theories about wellbeing. Finally, the marginal associations reveal that the association of religiosity with wellbeing is likely non-linear (Figure 2), and that the bivariate marginal associations of religiosity and cultural norms with wellbeing show no inkling of an interaction. So why is the interaction term significant in our planned confirmatory analyses? One potential explanation is that, through the high correlation between religiosity and cultural norms (r = .42), the interaction term captures some of the nonlinear effect of religiosity. These exploratory insights help us contextualize our confirmatory findings, provide alternative explanations, and suggest testable hypotheses for future confirmatory research. This is how rigorous exploration can complement confirmatory research.


Hastie, T., Tibshirani, R., & Friedman, J. (2009). The elements of statistical learning: Data mining, inference, and prediction (Second). Springer.

Peikert, A., Van Lissa, C. J., & Brandmaier, A. M. (2021, August 9). Reproducible Research in R: A tutorial on how to do the same thing more than once. https://doi.org/10.31234/osf.io/fwxs4

Van Lissa, C. J., Brandmaier, A. M., Brinkman, L., Lamprecht, A. L., Peikert, A., Struiksma, M. E., & Vreede, B. M. (2020). WORCS: A workflow for open reproducible code in science. Data Science, vol 4, no. 1, pp. 29-49. https://doi.org/10.3233/DS-210031
